{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CMPSC497 Fall 2022 Programming Assignment 2**\n",
        "Task 2\n",
        "Notebook by Anish Phule"
      ],
      "metadata": {
        "id": "3z2Vn7kVcHDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "1MtZzxYMcYbt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rsZsgpBx4DUv"
      },
      "outputs": [],
      "source": [
        "#Importing various libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import operator\n",
        "from math import ceil \n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This makes all figures appear bigger in the notebook\n",
        "plt.rcParams['figure.figsize'] = [10, 5]"
      ],
      "metadata": {
        "id": "17ZTIVjTdIMM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the input csv file\n",
        "df = pd.read_csv (r'/content/drive/MyDrive/CMPSC497/credit cards-2022-post.csv')\n",
        "df.to_excel (r'/content/drive/MyDrive/CMPSC497/credit_cards.xlsx', index = None, header=True)"
      ],
      "metadata": {
        "id": "IBACnPhCdLOK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clearing unclear or questionable values\n",
        "for index, row in df.iterrows():\n",
        "    if (row['EDUCATION'] < 1):\n",
        "        row['EDUCATION'] = 1\n",
        "    elif (row['EDUCATION'] >4):\n",
        "        row['EDUCATION'] = 4\n",
        "    if (row['MARRIAGE'] < 1):\n",
        "        row['MARRIAGE'] = 1\n",
        "  \n",
        "pay_list = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
        "for index, row in df.iterrows():\n",
        "    for i in range(len(pay_list)):\n",
        "        if (row[pay_list[i]] < 0):\n",
        "            row[pay_list[i]] = 0\n",
        "        \n",
        "df = df.reset_index()\n",
        "df.to_excel (r'/content/drive/MyDrive/CMPSC497/credit_cards_cleared.xlsx', index = None, header=True)"
      ],
      "metadata": {
        "id": "LwuxdhSriqlK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1(a): Creating new Features\n",
        "\n",
        "We create 6 new features, the Remaining balance, defined as 'Bill Amount - Pay Amount'. "
      ],
      "metadata": {
        "id": "cvs0NjlVpyf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bill_amt = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
        "pay_amt = ['PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
        "#We create 6 new features called Remaining Balance, which is the difference between bill amount and pay amount\n",
        "rem_bal = ['REM_BAL1','REM_BAL2','REM_BAL3','REM_BAL4','REM_BAL5','REM_BAL6']\n",
        "\n",
        "for i in range(6):\n",
        "    df[rem_bal[i]] = df[bill_amt[i]] - df[pay_amt[i]]\n",
        "\n",
        "df.to_excel (r'/content/drive/MyDrive/CMPSC497/credit_cards_updated.xlsx', index = None, header=True)\n",
        "print(df.head(3))"
      ],
      "metadata": {
        "id": "31ZrAHQhlzfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bad915-66a2-426f-a2fd-c591ccfc6369"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index    ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
            "0      0  5771     180000    1          2         1   38      1      2      2   \n",
            "1      1  3999      20000    2          1         2   23      2      2      2   \n",
            "2      2   397     100000    1          1         2   38      0      0      0   \n",
            "\n",
            "   ...  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  REM_BAL1  \\\n",
            "0  ...       124         0         0                           1       826   \n",
            "1  ...       500         0      1000                           1      6236   \n",
            "2  ...         0       579         0                           0     38308   \n",
            "\n",
            "   REM_BAL2  REM_BAL3  REM_BAL4  REM_BAL5  REM_BAL6  \n",
            "0      4412      4071      4947      5195         0  \n",
            "1     12559     11459     12457     13104     11744  \n",
            "2     41453     41491     43011     42432     28947  \n",
            "\n",
            "[3 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1(b): Principle Component Analysis\n",
        "\n",
        "We take the 18 features from Bill amount, Pay Amount and Remaining balance, and perform Principle Component Analysis to reduce the Dimensionality. "
      ],
      "metadata": {
        "id": "A8Cfx9CpicdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We first need to standardize our attributes for PCA\n",
        "attr = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6','REM_BAL1','REM_BAL2','REM_BAL3','REM_BAL4','REM_BAL5','REM_BAL6']\n",
        "df_pca = df[attr]\n",
        "\n",
        "for i in attr:\n",
        "  mean = df_pca[i].mean()\n",
        "  std = df_pca[i].std()\n",
        "  df_pca[i]=(df_pca[i]- mean)/std\n",
        "\n",
        "print(df_pca.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL4jQ-RuPzQ8",
        "outputId": "5922dcc8-31e6-4f02-cecd-13c20d4d38bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  \\\n",
            "0  -0.661082  -0.627831  -0.612095  -0.596497  -0.580089  -0.656619 -0.216708   \n",
            "1  -0.566379  -0.512940  -0.498629  -0.473366  -0.449176  -0.441485 -0.127637   \n",
            "2  -0.140501  -0.091370  -0.068644  -0.004104   0.045857  -0.167959 -0.186899   \n",
            "\n",
            "   PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  REM_BAL1  REM_BAL2  \\\n",
            "0 -0.237318 -0.274405 -0.294510 -0.310511 -0.291435 -0.614494 -0.532564   \n",
            "1 -0.237318 -0.245231 -0.270887 -0.310511 -0.235529 -0.539932 -0.420507   \n",
            "2 -0.196406 -0.253399 -0.302300 -0.273426 -0.291435 -0.097910 -0.023087   \n",
            "\n",
            "   REM_BAL3  REM_BAL4  REM_BAL5  REM_BAL6  \n",
            "0 -0.543989 -0.523671 -0.500633 -0.563364  \n",
            "1 -0.437804 -0.406328 -0.369513 -0.366943  \n",
            "2 -0.006167  0.071075  0.116704 -0.079220  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_pca\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=3, random_state=100)\n",
        "X = pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "3LGodY8ox5KT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.DataFrame(X, columns = ['A','B','C'])\n",
        "print(df_new.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy-X98DtXyTT",
        "outputId": "32f2a0e9-1bb4-478c-b047-55192453b38d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C\n",
            "0 -2.098848 -0.332509 -0.076862\n",
            "1 -1.650113 -0.298452 -0.108871\n",
            "2 -0.213877 -0.537090 -0.109664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task1(c): Normalization\n",
        "\n",
        "Before using the attributes for clustering, we must first normalize them. Here I am using Min-Max normalization, which involves subtracting the minimum value of an attribute, anddividing it by the range of the attribute. This brings all attribute values in between [0,1]."
      ],
      "metadata": {
        "id": "RSuhsX5EcZBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['A','B','C']\n",
        "for i in columns:\n",
        "  df_new[i] = (df_new[i] - df_new[i].min())/(df_new[i].max() - df_new[i].min())\n",
        "\n",
        "print(df_new.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwpDi2P9X5ZL",
        "outputId": "ab317caf-8cf4-40ed-bf97-a5a6c191f4bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C\n",
            "0  0.076566  0.035675  0.371407\n",
            "1  0.090043  0.036034  0.370660\n",
            "2  0.133176  0.033519  0.370642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: (Your own) Implementation of the Agglomerative Hierarchical Clustering (MIN) algorithm\n",
        "\n",
        "In this section, we implement our own hierarchal clustering using MIN criteria."
      ],
      "metadata": {
        "id": "_kMHQO2hdWY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task2(a): Represent the data points in a cluster using a data structure\n",
        "\n",
        "We must represent the data points in form of a data structure, that can effectively capture the hierarchal clustering of the data points.\\\n",
        "For this task, I am representing the points in the form of a nested list. I also create a parallel nested index list that houses the indexes of the points that are in a cluster together.\\\n",
        "In the beginning, since each point is a cluster, we will have one index in its individual list, and a list containing all these lists. We also have a separate list of lists with each list containing the attribute value for each point.\\\n",
        " When two points are clustered together, the index of the second point will be added to the list of the first point, and the second point's list will be removed. This goes on until until we have n_cluster remaining lists, with each list containing indexes of points that are clustered according to our proximity matrix.\n",
        "\n",
        "For example, for first 5 points, the index list would be [[1],[2],[3],[4],[5]]. If the points 2 and 3 are merged, the lists [2] and [3] will be merged too, and the list will now look like [[1],[2 , 3],[4],[5]]. This goes on until the no. of lists inside the list is equal to n_cluster.\n",
        "\n",
        "Illustration:\\\n",
        "step 1: Individual clusters - [ [1], [2], [3], [4], [5] ]\\\n",
        "step 2: cluster 2 and 3 merged - [ [1], [2 , 3], [4], [5] ]\\\n",
        "step 3: cluster 1 and 4 merged - [ [1 , 4],[2 , 3], [5] ]\\\n",
        "step 4: cluster [2 , 3] and 5 merged - [ [1 , 4], [2 , 3, 5] ]\\\n",
        "step 5: remaining 2 clusters merged - [ 1 , 4 , 2 , 3 , 5 ]\\\n",
        "Clustering done.\n",
        "\n",
        "And the beautiful thing of this structure is, the indexes are added in order of clustering. So, if we do intend to build a tree or a dendogram, we can backtrack the lists to travel from root to individual leaves. We might need to store the corresponding dissimilarities in a separate list so we know what should be the height in case of a dendogram, but it is completely doable.\n",
        "\n",
        "If we do intend to backtrack the clustering in order to know what were the clusters at what stage, we can create an additional list that appends the current clustering list to it every time clustering is performed. Thus we have a full record of the hierarchal clustering."
      ],
      "metadata": {
        "id": "zP3s4DaQdlhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of code block representation, I will be using only the first 5 data points to show code and results for section 2(a)-2(d). The full merged code with 100 points and 5 clusters is in section 2(e)."
      ],
      "metadata": {
        "id": "LkyN3RJSQQP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new dataframe of first 5 points\n",
        "df2 = df_new.head(5)\n",
        "\n",
        "# Create two empty lists, one each for the attribute values and index of data points\n",
        "Row_list =[]\n",
        "index_list = []\n",
        "  \n",
        "# Iterate over each row\n",
        "for index, rows in df2.iterrows():\n",
        "    # Create list for the current row\n",
        "    my_list =[rows.A, rows.B, rows.C]\n",
        "    my_index = [index]  \n",
        "\n",
        "    # append the list to the final list\n",
        "    Row_list.append(my_list)\n",
        "    index_list.append(my_index)"
      ],
      "metadata": {
        "id": "7Nn-yHwpYQxV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2(b): Proximity matrix\n",
        "\n",
        "We need to compute a proximity matrix for comparing the similarity/Dissimilarity of data points. For this project, we create a dissimilarity matrix in the form of a distance matrix. We calculate the total Euclidean distance between the attribute values of each point, and use it as a dissimilarity measure. Most dissimilar point will have the highest value in the matrix and vice versa.\n",
        "\n",
        "For this project, We use the distance_matrix function from the scipy library. This library takes in the attributes list of lists as input(as both x and y), and p=2 for euclidean distance. It then calculates pairwaise distance for each list in the attributes list. The great thing about this library, is that the x and y can be multidimensional, and the function can still easily calculate the euclidean distance."
      ],
      "metadata": {
        "id": "zMhsUsNaRIT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import important libraries\n",
        "import numpy as np\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "# compute the distance matrix\n",
        "dist_mat = distance_matrix(Row_list, Row_list, p=2)\n",
        "\n",
        "# display distance matrix\n",
        "print(\"Distance Matrix:\\n\", dist_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi_mEsOm0WOW",
        "outputId": "1cd0619f-ecc2-49c8-fda3-832f57b9ce42"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance Matrix:\n",
            " [[0.         0.01350205 0.05665639 0.13411989 0.30826433]\n",
            " [0.01350205 0.         0.04320683 0.12062529 0.29493524]\n",
            " [0.05665639 0.04320683 0.         0.07756498 0.2520509 ]\n",
            " [0.13411989 0.12062529 0.07756498 0.         0.17630309]\n",
            " [0.30826433 0.29493524 0.2520509  0.17630309 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2(c): Find two closest clusters based on the MIN/Single Link scheme\n",
        "\n",
        "For the MIN/Single link scheme, the two closest clusters are merged. We find the minimum value in the distance matrix(other than 0), and find the indices of that value in the matrix. These indices represent the clusters that are closest to each other. \n"
      ],
      "metadata": {
        "id": "q9upMw9qT-hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We first create a masked array that masks the 0.0 values, so we can concentrate on the non-zero ones\n",
        "masked_mat = np.ma.masked_equal(dist_mat, 0.0, copy=False)\n",
        "#We now find the minimum value in the matrix.\n",
        "#Note that since the min value will be in two locations, two index arrays will be output. Hence the (a,b)\n",
        "(a,b) = np.where(dist_mat == masked_mat.min())\n",
        "\n",
        "#Let's see the location\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPe3bL3Nhgss",
        "outputId": "f23d5639-bf43-42de-c8b7-6fed41aab0ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2(d): Merge two clusters into a new cluster and update the proximity matrix\n",
        "\n",
        "We now need to merge the two clusters found in the previous section into one, and update the proximity matrix. For both this tasks, we need not change the attribute lists. All clusterings from here on can be handled just on the basis of the previously calculated distance matrix.\n",
        "\n",
        "To merge the clusters, we simply add the lists corresponding to the indices obtained from the previous section. We then 'pop' the list for the latter index, since we don't need it anymore.\n",
        "\n",
        "For updating the proximity matrix, We compare the row and column values of distance matrix whose one index is the index of the either of location indexes found in the previous part. Then, both the rows/columns is equated to the minimum of the compared values, and one of the row and one of the column is deleted, since they are both the same."
      ],
      "metadata": {
        "id": "6HjmSoVsXBxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_list[a[0]] = index_list[a[0]] + index_list[a[1]] #Merging the index list of indices previously obtained\n",
        "index_list.pop(a[1]) #Popping the second list after it has been merged\n",
        "\n",
        "print(\"The merged Clusters are:\")\n",
        "print(index_list) #The list of lists after merging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYwyCWXgo4yr",
        "outputId": "f4230593-03dd-4ae5-f747-34d1eb71445e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The merged Clusters are:\n",
            "[[0, 1], [2], [3], [4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the lists 0 and 1 have been merged into one cluster.\n",
        "We now update the proximity matrix"
      ],
      "metadata": {
        "id": "WB8O49HKf91w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We now update the proximity matrix for indices 0 and 1, obtained previously.\n",
        "i = 0\n",
        "while i<len(index_list):\n",
        "  #Compare the row-wise values for 0 and 1, and equate it to the minimum one\n",
        "  dist_mat[a[0], i+1] = dist_mat[a[1], i+1] = min(dist_mat[a[0], i+1],dist_mat[a[1], i+1])\n",
        "  #Compare the column-wise values for 0 and 1, and equate it to the minimum one\n",
        "  dist_mat[i+1, a[0]] = dist_mat[i+1, a[1]] = min(dist_mat[i+1, a[0]],dist_mat[i+1, a[1]])\n",
        "  i+=1\n",
        "\n",
        "#Delete one column and row, since we now have two identical rows and two identical columns\n",
        "dist_mat = np.delete(dist_mat, a[0], 0)\n",
        "dist_mat = np.delete(dist_mat, a[0], 1)\n",
        "\n",
        "print(\"Updated Proximity matrix:\")\n",
        "print(dist_mat)"
      ],
      "metadata": {
        "id": "tLXDAP1LrxXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c1c210-f162-4645-817c-143a9d6aec1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Proximity matrix:\n",
            "[[0.         0.04320683 0.12062529 0.29493524]\n",
            " [0.04320683 0.         0.07756498 0.2520509 ]\n",
            " [0.12062529 0.07756498 0.         0.17630309]\n",
            " [0.29493524 0.2520509  0.17630309 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2(e): Implement your own version \n",
        "\n",
        "We now put together the above parts to create a complete Agglomerative Hierarchical Clustering (MIN) algorithm. \n",
        "\n",
        "In the beginning of the algorithm, we define the number of clusters we want and the number of points we want to cluster together. We also create a new column in the dataframe for assigning cluster labels after clustering is done.\n",
        "\n",
        "If we do intend to backtrack the clustering in order to know what were the clusters at what stage, we can create an additional list that appends the current clustering list to it every time clustering is performed. Thus we have a full record of the hierarchal clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "f1tGbe7Dftcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 5 #No. of clusters\n",
        "df2 = df_new.head(100) #No. of points to be clustered\n",
        "df2['labels'] = \"\" #Extra column added in dataframe for clusters.\n",
        "\n",
        "##Creating data structure for clustering - Task 2(a)\n",
        "\n",
        "# Create an empty list\n",
        "Row_list =[]\n",
        "index_list = []\n",
        "list_track = [] #adds the current cluster list to this incase we need to check clustering history\n",
        "\n",
        "# Iterate over each row\n",
        "for index, rows in df2.iterrows():\n",
        "    # Create list for the current row\n",
        "    my_list =[rows.A, rows.B, rows.C]\n",
        "    my_index = [index]  \n",
        "\n",
        "    # append the list to the final list\n",
        "    Row_list.append(my_list)\n",
        "    index_list.append(my_index)\n",
        "\n",
        "\n",
        "print(\"Initial Clusters:\\n\", index_list)\n",
        "#print(list_track)\n",
        "print(\"\\n\")\n",
        "##Calculating the Proximity Matrix - Task 2(b)\n",
        "\n",
        "# import important libraries\n",
        "import numpy as np\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "# compute the distance matrix\n",
        "dist_mat = distance_matrix(Row_list, Row_list, p=2)\n",
        "\n",
        "# display distance matrix\n",
        "print(\"Initial Distance Matrix:\\n\", dist_mat)\n",
        "\n",
        "##Finding the two closest clusters based on MIN scheme - Task 2(c)\n",
        "\n",
        "masked_mat = np.ma.masked_equal(dist_mat, 0.0, copy=False)\n",
        "(a,b) = np.where(dist_mat == masked_mat.min())\n",
        "\n",
        "##Merging two clusters into new cluster and updating proximity matrix - Task 2(d)\n",
        "\n",
        "index_list[a[0]] = index_list[a[0]] + index_list[a[1]]\n",
        "index_list.pop(a[1])\n",
        "\n",
        "list_track.append(index_list)\n",
        "\n",
        "k = len(dist_mat)\n",
        "while k>2:\n",
        "    if len(index_list) == n_clusters:   #n_cluster criterion\n",
        "      break\n",
        "    i = 0\n",
        "    while i<k-1:\n",
        "      dist_mat[a[0], i+1] = min(dist_mat[a[0], i+1],dist_mat[a[1], i+1])\n",
        "      dist_mat[a[1], i+1] = min(dist_mat[a[0], i+1],dist_mat[a[1], i+1])\n",
        "      dist_mat[i+1, a[0]] = dist_mat[i+1, a[1]] = min(dist_mat[i+1, a[0]],dist_mat[i+1, a[1]])\n",
        "      i+=1\n",
        "      \n",
        "    dist_mat = np.delete(dist_mat, a[0], 0)\n",
        "    dist_mat = np.delete(dist_mat, a[0], 1)\n",
        "\n",
        "    #We repeat the steps in task 2(b), 2(c) and 2(d)\n",
        "    masked_mat = np.ma.masked_equal(dist_mat, 0.0, copy=False)\n",
        "    (a,b) = np.where(dist_mat == masked_mat.min())\n",
        "    index_list[a[0]] = index_list[a[0]] + index_list[a[1]]\n",
        "    index_list.pop(a[1])\n",
        "    #list_track.append(index_list)\n",
        "    k-=1\n",
        "print(\"\\n\")\n",
        "print(\"Done\\n\")\n",
        "print(\"Final Proximity matrix\")\n",
        "print(dist_mat)\n",
        "print(\"\\n\")\n",
        "print(\"The clusters are(list of lists):\")\n",
        "[print(c) for c in index_list]\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "#Assigning labels to each data point based on their clustering\n",
        "for count in range(n_clusters):\n",
        "  for index in index_list[count]:\n",
        "    df2['labels'][index] = count+1\n",
        "print(\"First 10 data points with the attributes and labels:\\n\")\n",
        "print(df2.head(10))"
      ],
      "metadata": {
        "id": "QoXSpWzV7rt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8704facd-e38a-4bdd-ba5c-cbda09a16cd6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Clusters:\n",
            " [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99]]\n",
            "\n",
            "\n",
            "Initial Distance Matrix:\n",
            " [[0.         0.01350205 0.05665639 ... 0.03892663 0.756121   0.02340105]\n",
            " [0.01350205 0.         0.04320683 ... 0.02599499 0.742682   0.0190739 ]\n",
            " [0.05665639 0.04320683 0.         ... 0.01968244 0.70020361 0.04890605]\n",
            " ...\n",
            " [0.03892663 0.02599499 0.01968244 ... 0.         0.71852238 0.03639647]\n",
            " [0.756121   0.742682   0.70020361 ... 0.71852238 0.         0.7418233 ]\n",
            " [0.02340105 0.0190739  0.04890605 ... 0.03639647 0.7418233  0.        ]]\n",
            "\n",
            "\n",
            "Done\n",
            "\n",
            "Final Proximity matrix\n",
            "[[0.         0.57889815 0.53028434 0.48668546 0.67369914 0.45669749]\n",
            " [0.57889815 0.         0.4626182  0.10967473 0.76190287 0.        ]\n",
            " [0.53028434 0.4626182  0.         0.47077993 0.3102713  0.11447647]\n",
            " [0.48668546 0.10967473 0.47077993 0.         0.76460054 0.10224426]\n",
            " [0.67369914 0.76190287 0.3102713  0.76460054 0.         0.41977513]\n",
            " [0.45669749 0.         0.11447647 0.10224426 0.41977513 0.        ]]\n",
            "\n",
            "\n",
            "The clusters are(list of lists):\n",
            "[0, 28, 11, 7, 12, 82, 8, 75, 77, 45, 99, 34, 59, 25, 64, 37, 73, 76]\n",
            "[1, 33, 41, 74, 56, 93, 24]\n",
            "[2, 4, 69, 23, 80, 68, 50, 87, 3, 42, 54, 84, 61, 66, 79, 20, 26, 83, 86]\n",
            "[5, 65, 60, 49, 85, 91, 92, 98, 14, 17, 89, 62, 9, 94, 13, 40, 43, 15, 52, 35, 57, 71, 78, 88, 22, 63, 47, 72, 18, 38, 96, 48, 27, 44]\n",
            "[6, 31, 36, 29, 81, 46, 19, 16, 21, 39, 30, 70, 90, 51, 53, 55, 58, 95, 97, 32, 67]\n",
            "\n",
            "\n",
            "First 10 data points with the attributes and labels:\n",
            "\n",
            "          A         B         C labels\n",
            "0  0.076566  0.035675  0.371407      1\n",
            "1  0.090043  0.036034  0.370660      2\n",
            "2  0.133176  0.033519  0.370642      3\n",
            "3  0.210587  0.035690  0.366272      3\n",
            "4  0.383373  0.031999  0.401118      3\n",
            "5  0.113060  0.057194  0.339437      4\n",
            "6  0.075394  0.039807  0.369661      5\n",
            "7  0.120026  0.065475  0.329795      1\n",
            "8  0.093301  0.038744  0.375849      1\n",
            "9  0.177836  0.030946  0.378110      4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional things that can be done: Use list or similar data structure to append current index_list to it every time clustering is performed, to be able to see the clustering history"
      ],
      "metadata": {
        "id": "s5JslumbRmWI"
      }
    }
  ]
}